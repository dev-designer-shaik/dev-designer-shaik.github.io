<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>DSI Documentation ‚Äì Media Repository Architecture Guide</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header class="header">
        <div class="header__content">
            <div class="header__logo-container">
                <img src="logo.webp" alt="DSI Logo" class="header__logo">
                <button class="header__sidebar-toggle" id="sidebarToggle" aria-label="Toggle menu">Menu</button>
            </div>
            <div class="header__text">
                <h1 class="header__title">DSI IT Documentation</h1>
                <p class="header__subtitle">Media Repository Architecture & Workflow Guide</p>
            </div>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">üåô</button>
        </div>
    </header>

    <nav class="sidebar-nav" id="sidebarNav">
        <div class="nav-header">
            <button class="nav-collapse-btn" id="navCollapseBtn" aria-label="Collapse navigation">
                <span class="nav-collapse-icon">‚óÄ</span>
            </button>
        </div>
        <ul class="nav-links">
            <li class="nav-section-header">Overview & Governance</li>
            <li><a href="#system-overview" data-section="system-overview">System Overview</a></li>
            <li><a href="#governance" data-section="governance">Governance & Compliance</a></li>
            
            <li class="nav-section-header">Architecture & Design</li>
            <li><a href="#system-architecture" data-section="system-architecture">System Architecture</a></li>
            <li><a href="#repository-structure" data-section="repository-structure">Repository Structure</a></li>
            <li><a href="#database-design" data-section="database-design">Database Design</a></li>
            
            <li class="nav-section-header">Workflow Management</li>
            <li><a href="#event-processing" data-section="event-processing">Event Processing Pipeline</a></li>
            <li><a href="#approval-workflows" data-section="approval-workflows">Approval Workflows</a></li>
            <li><a href="#media-processing" data-section="media-processing">Media Processing</a></li>
            
            <li class="nav-section-header">Integration & Services</li>
            <li><a href="#external-integrations" data-section="external-integrations">External Integrations</a></li>
            <li><a href="#ai-intelligence" data-section="ai-intelligence">AI & Intelligence</a></li>
            
            <li class="nav-section-header">Implementation & Operations</li>
            <li><a href="#deployment-guide" data-section="deployment-guide">Deployment Guide</a></li>
            <li><a href="#monitoring-testing" data-section="monitoring-testing">Monitoring & Testing</a></li>
        </ul>
    </nav>

    <main class="main-content" id="main-content">

        <section id="system-overview" class="content-section">
            <h2 class="content-section__title">System Overview</h2>
            <p>The DSI Media Repository Integration Architecture is a comprehensive, AI-powered digital asset management ecosystem that automates the complete lifecycle of luxury product media from initial design concepts through production-ready marketing assets. This unified system consolidates all media processing, approval workflows, and distribution channels into a single, intelligent platform.</p>

            <h3 class="content-section__subtitle">Core Architecture Overview</h3>
            <div class="mermaid-container2">
                <div class="mermaid">
graph TB
    subgraph DL["Data Layer"]
        GD["üóÇÔ∏è Google Drive<br>Primary Source"]
        QNAP["üíæ QNAP NAS<br>Local Backup"]
        S3["‚òÅÔ∏è AWS S3/GCS<br>Cold Storage"]
    end

    subgraph PL["Processing Layer"]
        N8N["‚öôÔ∏è n8n<br>Workflow Engine"]
        OAI["ü§ñ OpenAI<br>AI Analysis"]
        PS["üì° Google Pub/Sub<br>Event Transport"]
        SA["üéØ Service Agents<br>K8s Orchestration"]
    end

    subgraph ST["Storage"]
        WV["üîç Weaviate<br>Vector Search"]
        PG["üóÑÔ∏è PostgreSQL<br>Metadata & Logs"]
    end
    
    subgraph DT["Distribution"]
        CDN["üåê Cloudinary<br>Media Optimization"]
        VJ["üìπ Vidjet<br>Video Hosting"]
        GC["üí¨ Google Chat<br>Notifications"]
    end
    
    subgraph BS["Business Systems"]
        ERP["üìä ERPNext<br>Product Management"]
        WC["üõí WooCommerce<br>eCommerce"]
        WF["üé® Webflow<br>Marketing"]
    end

    GD --> PS
    PS --> N8N
    N8N --> SA
    N8N --> OAI
    OAI --> WV
    N8N --> PG
    N8N --> CDN
    WV --> ERP
    CDN --> WC
    CDN --> WF
                </div>
            </div>

            <h3 class="content-section__subtitle">Technology Stack Matrix</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Category</th>
                        <th scope="col">Technology</th>
                        <th scope="col">Primary Function</th>
                        <th scope="col">Integration Points</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Category">Source Control</td><td data-label="Technology">Google Drive</td><td data-label="Primary Function">Primary media repository</td><td data-label="Integration Points">n8n triggers, AI processing</td></tr>
                    <tr><td data-label="Category">Message Broker</td><td data-label="Technology">RabbitMQ</td><td data-label="Primary Function">Event-driven workflow orchestration</td><td data-label="Integration Points">Queue management, workflow triggers</td></tr>
                    <tr><td data-label="Category">Orchestration</td><td data-label="Technology">n8n</td><td data-label="Primary Function">Workflow automation engine</td><td data-label="Integration Points">All systems integration hub</td></tr>
                    <tr><td data-label="Category">Vector Database</td><td data-label="Technology">Weaviate</td><td data-label="Primary Function">AI-powered semantic search</td><td data-label="Integration Points">OpenAI embeddings, metadata</td></tr>
                    <tr><td data-label="Category">Relational DB</td><td data-label="Technology">PostgreSQL</td><td data-label="Primary Function">Structured metadata storage</td><td data-label="Integration Points">Audit logs, processing status</td></tr>
                    <tr><td data-label="Category">AI Processing</td><td data-label="Technology">OpenAI</td><td data-label="Primary Function">Content analysis & tagging</td><td data-label="Integration Points">Weaviate vectors, metadata</td></tr>
                    <tr><td data-label="Category">CDN & Optimization</td><td data-label="Technology">Cloudinary</td><td data-label="Primary Function">Media transformation & delivery</td><td data-label="Integration Points">URL generation, format optimization</td></tr>
                    <tr><td data-label="Category">ERP</td><td data-label="Technology">ERPNext</td><td data-label="Primary Function">Product & inventory management</td><td data-label="Integration Points">SKU, pricing, BOM tracking</td></tr>
                    <tr><td data-label="Category">eCommerce</td><td data-label="Technology">WooCommerce</td><td data-label="Primary Function">Online product sales</td><td data-label="Integration Points">Product listings, media sync</td></tr>
                    <tr><td data-label="Category">Marketing</td><td data-label="Technology">Webflow</td><td data-label="Primary Function">Marketing websites</td><td data-label="Integration Points">Content management, CDN</td></tr>
                </tbody>
            </table>

            <h3 class="content-section__subtitle">Automation Capabilities Matrix</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Function Area</th>
                        <th scope="col">Tool / Agent</th>
                        <th scope="col">Automation Level</th>
                        <th scope="col">Key Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Function Area">Media Ingestion</td><td data-label="Tool / Agent">n8n + RabbitMQ</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Google Drive to processing pipeline</td></tr>
                    <tr><td data-label="Function Area">AI Tagging</td><td data-label="Tool / Agent">OpenAI + Weaviate</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Automatic metadata generation</td></tr>
                    <tr><td data-label="Function Area">Approval Management</td><td data-label="Tool / Agent">Multi-channel Approval</td><td data-label="Automation Level">Semi-Automated</td><td data-label="Key Use Case">Chat, Sheets, Forms integration</td></tr>
                    <tr><td data-label="Function Area">CDN Optimization</td><td data-label="Tool / Agent">Cloudinary</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Format conversion & delivery</td></tr>
                    <tr><td data-label="Function Area">ERP Integration</td><td data-label="Tool / Agent">n8n Scheduled Sync</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Product data synchronization</td></tr>
                    <tr><td data-label="Function Area">Document Processing</td><td data-label="Tool / Agent">Dual OCR Engine</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Text extraction & indexing</td></tr>
                    <tr><td data-label="Function Area">Quality Control</td><td data-label="Tool / Agent">AI Quality Assessment</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Automated quality scoring</td></tr>
                    <tr><td data-label="Function Area">Publication</td><td data-label="Tool / Agent">Multi-platform Sync</td><td data-label="Automation Level">Fully Automated</td><td data-label="Key Use Case">Cross-platform distribution</td></tr>
                </tbody>
            </table>
        </section>

        <section id="governance" class="content-section">
            <h2 class="content-section__title">Governance & Compliance</h2>
            
            <h3 class="content-section__subtitle">IP Hierarchy</h3>
            <div class="mermaid-container7">
                <div class="mermaid">
%%{init: {
  "theme": "base",
  "themeCSS": ".edgePath marker path { fill: #808080 !important; stroke: #536277 !important; }\n.marker { fill: #808080 !important; }\n.text-item-align { fill: #808080 !important; }\n.subgraph .title { fill: #808080 !important; }"
}}%%
graph TB
    MH["Marayai Holdings<br>Parent Company"]
    SF["Shaikdom Foundation<br>Non-Profit Arm"]
    DSS["Designer Shaik Studio<br>Creative Unit"]
    DSI["DSI Product Repository<br>Content Hub"]
    
    MH --> SF
    MH --> DSS
    DSS --> DSI
    SF --> DSI
                </div>
            </div>

            <h3 class="content-section__subtitle">Digital Asset Ownership</h3>
            <p>All digital assets created or acquired by any entity within the Marayai ecosystem are ultimately owned by Marayai Holdings. Usage rights, licensing, and distribution are governed by specific protocols outlined in this guide. The system maintains comprehensive audit trails for all asset modifications, approvals, and distributions to ensure compliance with intellectual property regulations.</p>
            
            <h3 class="content-section__subtitle">Compliance Framework</h3>
            <p>The architecture adheres to regional data privacy laws including GDPR and ADGM regulations. All processing activities are logged in PostgreSQL with immutable audit trails. Regular compliance audits are conducted through automated reporting systems integrated with the workflow engine.</p>

            <h3 class="content-section__subtitle">Data Governance Policies</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Data Category</th>
                        <th scope="col">Retention Policy</th>
                        <th scope="col">Access Control</th>
                        <th scope="col">Backup Strategy</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Data Category">Product Media</td><td data-label="Retention Policy">Permanent</td><td data-label="Access Control">Role-based (Design Team)</td><td data-label="Backup Strategy">Multi-tier (Drive, NAS, Cloud)</td></tr>
                    <tr><td data-label="Data Category">Processing Logs</td><td data-label="Retention Policy">7 years</td><td data-label="Access Control">IT Admin only</td><td data-label="Backup Strategy">PostgreSQL automated backup</td></tr>
                    <tr><td data-label="Data Category">Approval Records</td><td data-label="Retention Policy">Permanent</td><td data-label="Access Control">Management + Legal</td><td data-label="Backup Strategy">Immutable audit trail</td></tr>
                    <tr><td data-label="Data Category">AI Metadata</td><td data-label="Retention Policy">Linked to source asset</td><td data-label="Access Control">System automated</td><td data-label="Backup Strategy">Weaviate cluster backup</td></tr>
                </tbody>
            </table>
        </section>

        <section id="system-architecture" class="content-section">
            <h2 class="content-section__title">System Architecture</h2>
            
            <h3 class="content-section__subtitle">Core Components</h3>
            <div class="mermaid-container5">
                <div class="mermaid">
%%{init: {
  "theme": "base",
  "themeCSS": ".edgePath marker path { fill: #808080 !important; stroke: #536277 !important; }\n.marker { fill: #808080 !important; }\n.text-item-align { fill: #808080 !important; }\n.subgraph .title { fill: #808080 !important; }"
}}%%
graph TB
    subgraph Input["Data Input Layer"]
        GD["Google Drive<br>Primary Repository"]
        PS["Google Pub/Sub<br>Event Stream"]
        CF["Cloud Function<br>Webhook Handler"]
        RMQ["RabbitMQ<br>Message Broker"]
    end

    subgraph Processing["Processing Layer"]
        N8N["n8n<br>Workflow Engine"]
        OAI["OpenAI API<br>Vision & Analysis"]
        OCR["Dual OCR<br>Textract + Vision"]
    end

    subgraph Storage["Storage Layer"]
        PG["PostgreSQL<br>Metadata & Logs"]
        WV["Weaviate<br>Vector Database"]
        CDN["Cloudinary<br>Media CDN"]
    end

    subgraph Distribution["Distribution Layer"]
        ERP["ERPNext<br>Product Data"]
        WC["WooCommerce<br>eCommerce"]
        WF["Webflow<br>Marketing Sites"]
    end

    GD --> PS
    PS --> CF
    CF --> RMQ
    RMQ --> N8N
    N8N --> OAI
    N8N --> OCR
    N8N --> PG
    OAI --> WV
    N8N --> CDN
    CDN --> ERP
    CDN --> WC
    CDN --> WF
                </div>
            </div>

            <h3 class="content-section__subtitle">Data Flow Architecture</h3>
            <p>The system implements a sophisticated event-driven architecture where Google Drive serves as the primary source of truth. File changes trigger webhook notifications that flow through Google Pub/Sub to Cloud Functions, which then publish structured messages to RabbitMQ queues. This design ensures reliable message delivery and enables sophisticated routing based on file types and processing requirements.</p>

            <h3 class="content-section__subtitle">Processing Pipeline</h3>
            <pre class="code-block">
1. File Upload ‚Üí Google Drive
2. Drive API ‚Üí Webhook Notification  
3. Cloud Function ‚Üí Pub/Sub Message
4. RabbitMQ ‚Üí Queue Routing
5. n8n Trigger ‚Üí Workflow Execution
6. AI Processing ‚Üí Metadata Extraction
7. Database Storage ‚Üí PostgreSQL + Weaviate
8. CDN Optimization ‚Üí Cloudinary
9. System Sync ‚Üí ERPNext, WooCommerce, Webflow
            </pre>

            <h3 class="content-section__subtitle">Scalability Considerations</h3>
            <p>The architecture is designed for horizontal scaling with RabbitMQ clustering for high availability message processing, PostgreSQL read replicas for query performance, and Weaviate distributed deployment for vector search scalability. Cloudinary provides global CDN distribution with automatic scaling based on demand.</p>
        </section>

        <section id="repository-structure" class="content-section">
            <h2 class="content-section__title">Repository Structure</h2>
            
            <h3 class="content-section__subtitle">Repository Folder Structure</h3>
            <div class="diagram-split">

                <div class="diagram-part">
                    <h4>Processed Media Structure</h4>
                    <div class="mermaid-container1">
                        <div class="mermaid">
%%{init: {
  "theme": "base",
  "themeCSS": ".edgePath marker path { fill: #808080 !important; stroke: #536277 !important; }\n.marker { fill: #808080 !important; }\n.text-item-align { fill: #808080 !important; }\n.subgraph .title { fill: #808080 !important; }"
}}%%
graph TB
    PM["üìÅ Processed-Media"]
    
    subgraph ERP["ERP-Integrated"]
        SKU_Prod["[SKU]_[ProductName]_[Version]"]
    end
    
    subgraph Web["Web-Optimized"]
        Cloudinary["Cloudinary-URLs"]
        CDN["CDN-Manifests"]
    end
    
    subgraph Social["Social-Media-Posts"]
        Platform_Prod["[Platform]_[ProductName]_[Date]"]
    end
    
    PM --> ERP
    PM --> Web
    PM --> Social
                        </div>
                    </div>
                </div>
            </div>

            <h3 class="content-section__subtitle">Google Drive Hierarchy</h3>
            <pre class="code-block">
/DSI-Product-Repository/
‚îú‚îÄ‚îÄ Palace-of-Fragrances/
‚îÇ   ‚îú‚îÄ‚îÄ Classic-Range/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [Product-Name]/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 00-Early-Design/         # Concept Stage
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Research/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Sketches/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Concept-CAD/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 01-Design-Review/        # Gate-A Stage
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Feasibility/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Approvals/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 02-Prototype/            # Prototype Stage
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SOLIDWORKS/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 3D-Prints/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ BOM/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 03-Quality/              # PPAP Stage
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Testing/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Certifications/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 04-Production/           # Production Stage
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Final-Media/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Marketing-Assets/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Metadata/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ MCP-Registry/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ Event-History/
‚îÇ   ‚îî‚îÄ‚îÄ Precious-Range/
‚îú‚îÄ‚îÄ Palace-of-Precision/
‚îî‚îÄ‚îÄ Palace-of-Style/
            </pre>

            <h3 class="content-section__subtitle">File Naming Convention</h3>
            <div class="highlight-box">
                <strong>Pattern:</strong> <code>{ProductName}_{SKU}_{descriptor}_{timestamp}.{extension}</code>
                <ul>
                    <li><code>Opulent-Shaik-Amethyst_OSA-GE-100_hero_20250703.jpg</code></li>
                    <li><code>AstroTime-MK1_ATM-IG-001_3d_render.blend</code></li>
                    <li><code>Objet-d-Art-Optimum_OAO-FL-50_metadata.json</code></li>
                </ul>
            </div>

            <h3 class="content-section__subtitle">MCP Protocol Integration</h3>
            <p>The Model Context Protocol (MCP) provides standardized metadata management across all assets. Each file receives an MCP ID following the pattern <code>[ProductName]_[Version]_[Status]_[Timestamp]</code>, enabling intelligent version control and context tracking throughout the processing pipeline.</p>
        </section>

        <section id="database-design" class="content-section">
            <h2 class="content-section__title">Database Design</h2>

            <h3 class="content-section__subtitle">PostgreSQL Schema Visualization</h3>
            <div class="mermaid-container1">
                <div class="mermaid" id="media-assets-er-diagram">
erDiagram
    folder_structure {
        SERIAL    id               PK
        VARCHAR   drive_folder_id  UK
        TEXT      folder_name
        TEXT      full_path        "e.g. /assets/images/2025/07"
        TIMESTAMP last_synced_at
        TIMESTAMP created_at
        TIMESTAMP updated_at
    }

    media_assets {
        SERIAL    id               PK
        UUID      weaviate_object_id
        VARCHAR   drive_file_id    UK
        VARCHAR   mcp_id
        VARCHAR   product_name
        VARCHAR   sku
        VARCHAR   file_name
        BIGINT    file_size
        VARCHAR   mime_type
        INTEGER   parent_id        FK  "‚Üí folder_structure.id"
        TEXT      cloudinary_url
        VARCHAR   processing_status
        TEXT      ai_description
        TEXT      ai_tags
        TIMESTAMP created_at
        TIMESTAMP updated_at
    }

    processing_logs {
        SERIAL    id               PK
        INTEGER   media_asset_id   FK  "‚Üí media_assets.id"
        VARCHAR   workflow_id
        VARCHAR   step_name
        VARCHAR   status
        TEXT      message
        TIMESTAMP created_at
    }

    approval_records {
        SERIAL    id               PK
        INTEGER   media_asset_id   FK  "‚Üí media_assets.id"
        VARCHAR   approver_email
        VARCHAR   approval_status
        TEXT      comments
        TIMESTAMP approved_at
    }

    sync_status {
        SERIAL    id               PK
        INTEGER   media_asset_id   FK  "‚Üí media_assets.id"
        VARCHAR   target_system
        VARCHAR   sync_status
        TIMESTAMP last_sync_at
        TEXT      error_message
    }

    plm_transitions {
        SERIAL    id               PK
        INTEGER   media_asset_id   FK  "‚Üí media_assets.id"
        VARCHAR   from_stage
        VARCHAR   to_stage
        VARCHAR   triggered_by
        VARCHAR   transition_type
        VARCHAR   approval_by
        TEXT      comments
        VARCHAR   rabbitmq_message_id
        TIMESTAMP created_at
    }

    event_history {
        SERIAL    id               PK
        VARCHAR   event_id         UK
        VARCHAR   exchange
        VARCHAR   routing_key
        JSONB     payload
        BOOLEAN   processed
        TIMESTAMP processed_at
        TEXT      error_message
        INTEGER   retry_count
        TIMESTAMP created_at
    }

    solidworks_metadata {
        SERIAL    id               PK
        INTEGER   media_asset_id   FK  "‚Üí media_assets.id"
        VARCHAR   part_number
        VARCHAR   material
        DECIMAL   weight
        JSONB     dimensions
        JSONB     custom_properties
        JSONB     bom_data
        TIMESTAMP extracted_at
    }

    document_index {
        SERIAL    id               PK
        INTEGER   media_asset_id   FK  "‚Üí media_assets.id"
        VARCHAR   mcp_id
        VARCHAR   doc_type
        VARCHAR   language
        TEXT      full_text
        TEXT[]    key_terms
        JSONB     extracted_data
        DECIMAL   ocr_confidence
        TEXT      drive_url
        TEXT      cloudinary_url
        TIMESTAMP created_at
    }

    approval_matrix {
        SERIAL    id               PK
        VARCHAR   plm_stage
        VARCHAR   approver_role
        VARCHAR   approver_email
        INTEGER   approval_order
        BOOLEAN   is_mandatory
        TIMESTAMP created_at
    }

    %% Relationships
    folder_structure ||--o{ media_assets         : contains
    media_assets      ||--o{ processing_logs     : logs
    media_assets      ||--o{ approval_records    : approvals
    media_assets      ||--o{ sync_status         : sync
    media_assets      ||--o{ plm_transitions     : transitions
    media_assets      ||--o{ solidworks_metadata : solidworks_data
    media_assets      ||--o{ document_index      : document_data

                </div>
            </div>

            <h3 class="content-section__subtitle">Redis Cache Architecture</h3>
            <div class="mermaid-container">
                <div class="mermaid">
graph TB
    subgraph "Queue Keys"
        QPM["queue:media:pending<br/>üìã Pending files"]
        QPR["queue:media:processing<br/>‚öôÔ∏è Active processing"]
        QPF["queue:media:failed<br/>‚ùå Failed attempts"]
    end

    subgraph "Cache Keys"
        CF["cache:folder:ID<br/>üìÅ Folder metadata"]
        CP["cache:product:SKU<br/>üè∑Ô∏è Product info"]
        CC["cache:cloudinary:ID<br/>üåê CDN URLs"]
    end

    subgraph "Lock Keys"
        LF["lock:file:ID<br/>üîí Processing lock"]
        LW["lock:workflow:ID<br/>‚ö° Workflow lock"]
    end
                </div>
            </div>

            <h3 class="content-section__subtitle">Database Schema Implementation</h3>
            <pre class="code-block">
-- Media Assets Table
CREATE TABLE media_assets (
    id SERIAL PRIMARY KEY,
    drive_file_id VARCHAR(255) UNIQUE NOT NULL,
    weaviate_object_id UUID,
    mcp_id VARCHAR(255) UNIQUE NOT NULL,
    product_name VARCHAR(255) NOT NULL,
    product_line VARCHAR(100),
    product_range VARCHAR(100),
    sku VARCHAR(100),
    file_url TEXT NOT NULL,
    cloudinary_url TEXT,
    mime_type VARCHAR(100),
    file_size BIGINT,
    processing_status VARCHAR(50) DEFAULT 'pending',
    plm_stage VARCHAR(50) DEFAULT 'Draft',
    designer_id VARCHAR(100),
    revision VARCHAR(20) DEFAULT 'A00',
    linked_bom VARCHAR(100),
    linked_eco VARCHAR(100),
    ai_description TEXT,
    ai_tags TEXT[],
    quality_score DECIMAL(3,2),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    published_at TIMESTAMP,
    archived_at TIMESTAMP
);

-- Processing Logs Table
CREATE TABLE processing_logs (
    id SERIAL PRIMARY KEY,
    media_asset_id INTEGER REFERENCES media_assets(id),
    workflow_id VARCHAR(100),
    step_name VARCHAR(100),
    status VARCHAR(50),
    message TEXT,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Approval Records Table
CREATE TABLE approval_records (
    id SERIAL PRIMARY KEY,
    media_asset_id INTEGER REFERENCES media_assets(id),
    approver_email VARCHAR(255),
    approval_status VARCHAR(50),
    comments TEXT,
    approved_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Sync Status Table
CREATE TABLE sync_status (
    id SERIAL PRIMARY KEY,
    media_asset_id INTEGER REFERENCES media_assets(id),
    target_system VARCHAR(50), -- 'ERPNEXT', 'WOOCOMMERCE', 'WEBFLOW'
    sync_status VARCHAR(50),
    last_sync_at TIMESTAMP,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0
);

-- PLM Transitions Table
CREATE TABLE plm_transitions (
    id SERIAL PRIMARY KEY,
    media_asset_id INTEGER REFERENCES media_assets(id),
    from_stage VARCHAR(50),
    to_stage VARCHAR(50),
    triggered_by VARCHAR(100),
    transition_type VARCHAR(50), -- 'manual', 'automated', 'approval'
    approval_by VARCHAR(100),
    comments TEXT,
    rabbitmq_message_id VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Event History Table
CREATE TABLE event_history (
    id SERIAL PRIMARY KEY,
    event_id VARCHAR(255) UNIQUE NOT NULL,
    exchange VARCHAR(100),
    routing_key VARCHAR(255),
    payload JSONB,
    processed BOOLEAN DEFAULT FALSE,
    processed_at TIMESTAMP,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW()
);

-- SOLIDWORKS Metadata Table
CREATE TABLE solidworks_metadata (
    id SERIAL PRIMARY KEY,
    media_asset_id INTEGER REFERENCES media_assets(id),
    part_number VARCHAR(100),
    material VARCHAR(100),
    weight DECIMAL(10,3),
    dimensions JSONB,
    custom_properties JSONB,
    bom_data JSONB,
    extracted_at TIMESTAMP DEFAULT NOW()
);

-- Document Index Table (OCR)
CREATE TABLE document_index (
    id SERIAL PRIMARY KEY,
    media_asset_id INTEGER REFERENCES media_assets(id),
    mcp_id VARCHAR(255),
    doc_type VARCHAR(100),
    language VARCHAR(10),
    full_text TEXT,
    key_terms TEXT[],
    extracted_data JSONB,
    ocr_confidence DECIMAL(3,2),
    drive_url TEXT,
    cloudinary_url TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Approval Matrix Table
CREATE TABLE approval_matrix (
    id SERIAL PRIMARY KEY,
    plm_stage VARCHAR(50),
    approver_role VARCHAR(100),
    approver_email VARCHAR(255),
    approval_order INTEGER,
    is_mandatory BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Create Indexes
CREATE INDEX idx_media_assets_sku ON media_assets(sku);
CREATE INDEX idx_media_assets_mcp ON media_assets(mcp_id);
CREATE INDEX idx_media_assets_plm_stage ON media_assets(plm_stage);
CREATE INDEX idx_media_assets_status ON media_assets(processing_status);
CREATE INDEX idx_plm_transitions_asset ON plm_transitions(media_asset_id);
CREATE INDEX idx_event_history_processed ON event_history(processed);
CREATE INDEX idx_document_index_mcp ON document_index(mcp_id);
            </pre>

            <h3 class="content-section__subtitle">Weaviate Cluster Schema Visualization</h3>
            <div class="mermaid-container1"> 
                <div class="mermaid" id="weaviate-er-diagram">
erDiagram

    Image {
        STRING    productName      "Product name from folder structure"
        STRING    sku              "Stock keeping unit identifier"
        TEXT      description      "AI-generated content description"
        STRING[]  tags             "AI-generated semantic tags"
        STRING[]  colors           "Detected color palette"
        NUMBER    qualityScore     "AI-assessed quality rating (0-10)"
        STRING    mcpId            "Model Context Protocol identifier"
        STRING    resolution       "Image resolution (e.g., 1920x1080)"
        STRING    fileFormat       "File format (e.g., JPG, PNG)"
    }

    Video {
        STRING    productName      "Product name from folder structure"
        STRING    sku              "Stock keeping unit identifier"
        TEXT      description      "AI-generated content description"
        STRING[]  tags             "AI-generated semantic tags"
        NUMBER    duration         "Video duration in seconds"
        STRING    resolution       "Video resolution (e.g., 1920x1080)"
        STRING    fileFormat       "File format (e.g., MP4, MOV)"
        STRING    mcpId            "Model Context Protocol identifier"
    }

    Audio {
        STRING    productName      "Product name from folder structure"
        STRING    sku              "Stock keeping unit identifier"
        TEXT      description      "AI-generated content description"
        STRING[]  tags             "AI-generated semantic tags"
        NUMBER    duration         "Audio duration in seconds"
        STRING    fileFormat       "File format (e.g., MP3, WAV)"
        STRING    mcpId            "Model Context Protocol identifier"
    }

    Document {
        STRING    productName      "Product name from folder structure"
        STRING    sku              "Stock keeping unit identifier"
        TEXT      description      "AI-generated content description"
        STRING[]  tags             "AI-generated semantic tags"
        STRING    fileFormat       "File format (e.g., PDF, DOCX)"
        TEXT      fullText         "Extracted full text content"
        STRING    mcpId            "Model Context Protocol identifier"
    }

    ThreeDModel {
        STRING    productName      "Product name from folder structure"
        STRING    sku              "Stock keeping unit identifier"
        TEXT      description      "AI-generated content description"
        STRING[]  tags             "AI-generated semantic tags"
        STRING    fileFormat       "File format (e.g., OBJ, FBX, GLB)"
        NUMBER    polygonCount     "Number of polygons in the 3D model"
        NUMBER    textureCount     "Number of textures in the 3D model"
        STRING    mcpId            "Model Context Protocol identifier"
    }


                </div>
            </div>

            <h3 class="content-section__subtitle">Weaviate Cluster Configuration</h3>
            <pre class="code-block">
# Weaviate Cluster Setup
version: '3.8'

services:
  weaviate:
    image: semitechnologies/weaviate:latest
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=false
      - AUTHENTICATION_APIKEY_ENABLED=true
      - AUTHENTICATION_APIKEY_ALLOWED_KEYS=${WEAVIATE_API_KEY}
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - DEFAULT_VECTORIZER_MODULE=text2vec-openai
      - ENABLE_MODULES=text2vec-openai,text2vec-transformers,qna-openai
      - OPENAI_APIKEY=${OPENAI_API_KEY}
      - CLUSTER_HOSTNAME=weaviate-node-1
      - CLUSTER_GOSSIP_BIND_PORT=7100
      - CLUSTER_DATA_BIND_PORT=7101
    volumes:
      - weaviate_data:/var/lib/weaviate

  weaviate-node-2:
    image: semitechnologies/weaviate:latest
    restart: unless-stopped
    ports:
      - "8081:8080"
    environment:
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=false
      - AUTHENTICATION_APIKEY_ENABLED=true
      - AUTHENTICATION_APIKEY_ALLOWED_KEYS=${WEAVIATE_API_KEY}
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - DEFAULT_VECTORIZER_MODULE=text2vec-openai
      - ENABLE_MODULES=text2vec-openai,text2vec-transformers,qna-openai
      - OPENAI_APIKEY=${OPENAI_API_KEY}
      - CLUSTER_HOSTNAME=weaviate-node-2
      - CLUSTER_GOSSIP_BIND_PORT=7100
      - CLUSTER_DATA_BIND_PORT=7101
      - CLUSTER_JOIN=weaviate:7100
    volumes:
      - weaviate_data_2:/var/lib/weaviate
    depends_on:
      - weaviate

volumes:
  weaviate_data:
  weaviate_data_2:
            </pre>

            <h3 class="content-section__subtitle">Semantic Search Implementation</h3>
            <pre class="code-block">
// Weaviate Client Configuration
import weaviate from 'weaviate-ts-client';

const client = weaviate.client({
  scheme: 'http',
  host: 'localhost:8080',
  apiKey: new weaviate.ApiKey(process.env.WEAVIATE_API_KEY),
  headers: { 'X-OpenAI-Api-Key': process.env.OPENAI_API_KEY },
});

// Schema Creation for All Asset Types
const createSchemas = async () => {
  const schemas = [
    {
      class: 'Image',
      description: 'Image assets with AI-generated metadata',
      vectorizer: 'text2vec-openai',
      properties: [
        { name: 'productName', dataType: ['string'] },
        { name: 'sku', dataType: ['string'] },
        { name: 'description', dataType: ['text'] },
        { name: 'tags', dataType: ['string[]'] },
        { name: 'colors', dataType: ['string[]'] },
        { name: 'qualityScore', dataType: ['number'] },
        { name: 'mcpId', dataType: ['string'] },
        { name: 'resolution', dataType: ['string'] },
        { name: 'fileFormat', dataType: ['string'] }
      ]
    },
    {
      class: 'Video',
      description: 'Video assets with AI-generated metadata',
      vectorizer: 'text2vec-openai',
      properties: [
        { name: 'productName', dataType: ['string'] },
        { name: 'sku', dataType: ['string'] },
        { name: 'description', dataType: ['text'] },
        { name: 'tags', dataType: ['string[]'] },
        { name: 'duration', dataType: ['number'] },
        { name: 'resolution', dataType: ['string'] },
        { name: 'fileFormat', dataType: ['string'] },
        { name: 'mcpId', dataType: ['string'] }
      ]
    },
    {
      class: 'Audio',
      description: 'Audio assets with AI-generated metadata',
      vectorizer: 'text2vec-openai',
      properties: [
        { name: 'productName', dataType: ['string'] },
        { name: 'sku', dataType: ['string'] },
        { name: 'description', dataType: ['text'] },
        { name: 'tags', dataType: ['string[]'] },
        { name: 'duration', dataType: ['number'] },
        { name: 'fileFormat', dataType: ['string'] },
        { name: 'mcpId', dataType: ['string'] }
      ]
    },
    {
      class: 'Document',
      description: 'Document assets with AI-generated metadata',
      vectorizer: 'text2vec-openai',
      properties: [
        { name: 'productName', dataType: ['string'] },
        { name: 'sku', dataType: ['string'] },
        { name: 'description', dataType: ['text'] },
        { name: 'tags', dataType: ['string[]'] },
        { name: 'fileFormat', dataType: ['string'] },
        { name: 'fullText', dataType: ['text'] },
        { name: 'mcpId', dataType: ['string'] }
      ]
    },
    {
      class: 'ThreeDModel',
      description: '3D model assets with AI-generated metadata',
      vectorizer: 'text2vec-openai',
      properties: [
        { name: 'productName', dataType: ['string'] },
        { name: 'sku', dataType: ['string'] },
        { name: 'description', dataType: ['text'] },
        { name: 'tags', dataType: ['string[]'] },
        { name: 'fileFormat', dataType: ['string'] },
        { name: 'polygonCount', dataType: ['number'] },
        { name: 'textureCount', dataType: ['number'] },
        { name: 'mcpId', dataType: ['string'] }
      ]
    }
  ];

  for (const schema of schemas) {
    await client.schema.classCreator().withClass(schema).do();
  }
};

// Semantic Search Functions
const searchImages = async (query, limit = 10) => {
  return await client.graphql
    .get()
    .withClassName('Image')
    .withFields('productName sku description tags colors qualityScore mcpId resolution fileFormat')
    .withNearText({ concepts: [query] })
    .withLimit(limit)
    .do();
};

const searchVideos = async (query, limit = 10) => {
  return await client.graphql
    .get()
    .withClassName('Video')
    .withFields('productName sku description tags duration resolution fileFormat mcpId')
    .withNearText({ concepts: [query] })
    .withLimit(limit)
    .do();
};

const searchAudio = async (query, limit = 10) => {
  return await client.graphql
    .get()
    .withClassName('Audio')
    .withFields('productName sku description tags duration fileFormat mcpId')
    .withNearText({ concepts: [query] })
    .withLimit(limit)
    .do();
};

const searchDocuments = async (query, limit = 10) => {
  return await client.graphql
    .get()
    .withClassName('Document')
    .withFields('productName sku description tags fileFormat fullText mcpId')
    .withNearText({ concepts: [query] })
    .withLimit(limit)
    .do();
};

const search3DModels = async (query, limit = 10) => {
  return await client.graphql
    .get()
    .withClassName('ThreeDModel')
    .withFields('productName sku description tags fileFormat polygonCount textureCount mcpId')
    .withNearText({ concepts: [query] })
    .withLimit(limit)
    .do();
};

// Cross-Asset Type Search
const searchAllAssets = async (query, limit = 50) => {
  const [images, videos, audio, documents, models] = await Promise.all([
    searchImages(query, limit),
    searchVideos(query, limit),
    searchAudio(query, limit),
    searchDocuments(query, limit),
    search3DModels(query, limit)
  ]);

  return {
    images: images.data.Get.Image || [],
    videos: videos.data.Get.Video || [],
    audio: audio.data.Get.Audio || [],
    documents: documents.data.Get.Document || [],
    models: models.data.Get.ThreeDModel || []
  };
};

// Advanced Filtering
const searchWithFilters = async (query, assetType, filters = {}) => {
  let graphqlQuery = client.graphql
    .get()
    .withClassName(assetType)
    .withNearText({ concepts: [query] });

  // Add where filters
  if (Object.keys(filters).length > 0) {
    const whereFilter = {
      operator: 'And',
      operands: Object.entries(filters).map(([key, value]) => ({
        path: [key],
        operator: 'Equal',
        valueString: value
      }))
    };
    graphqlQuery = graphqlQuery.withWhere(whereFilter);
  }

  return await graphqlQuery.do();
};

// Hybrid Search (Vector + Keyword)
const hybridSearch = async (query, assetType, alpha = 0.7) => {
  return await client.graphql
    .get()
    .withClassName(assetType)
    .withHybrid({
      query: query,
      alpha: alpha // 0 = pure keyword, 1 = pure vector
    })
    .do();
};

// Export functions
export {
  createSchemas,
  searchImages,
  searchVideos,
  searchAudio,
  searchDocuments,
  search3DModels,
  searchAllAssets,
  searchWithFilters,
  hybridSearch
};
            </pre>

            <h3 class="content-section__subtitle">Asset Ingestion Pipeline</h3>
            <pre class="code-block">
// Asset Ingestion to Weaviate
const ingestAsset = async (assetData, assetType) => {
  try {
    // Generate embeddings using OpenAI
    const embedding = await generateEmbedding(assetData.description);
    
    // Create object in Weaviate
    const result = await client.data
      .creator()
      .withClassName(assetType)
      .withProperties(assetData)
      .withVector(embedding)
      .do();
    
    console.log(`${assetType} asset ingested:`, result.id);
    return result;
  } catch (error) {
    console.error(`Error ingesting ${assetType}:`, error);
    throw error;
  }
};

// Batch ingestion for performance
const batchIngestAssets = async (assets) => {
  const batcher = client.batch.objectsBatcher();
  
  for (const asset of assets) {
    batcher.withObject({
      class: asset.type,
      properties: asset.data,
      vector: asset.embedding
    });
  }
  
  return await batcher.do();
};

// Real-time sync with PostgreSQL
const syncWithPostgreSQL = async (mediaAssetId) => {
  // Fetch from PostgreSQL
  const pgResult = await pg.query(
    'SELECT * FROM media_assets WHERE id = $1',
    [mediaAssetId]
  );
  
  if (pgResult.rows.length === 0) return;
  
  const asset = pgResult.rows[0];
  const assetType = determineAssetType(asset.mime_type);
  
  // Prepare data for Weaviate
  const weaviateData = {
    productName: asset.product_name,
    sku: asset.sku,
    description: asset.ai_description,
    tags: asset.ai_tags,
    mcpId: asset.mcp_id,
    ...getTypeSpecificFields(asset, assetType)
  };
  
  // Ingest to Weaviate
  await ingestAsset(weaviateData, assetType);
};

// Helper functions
const determineAssetType = (mimeType) => {
  if (mimeType.startsWith('image/')) return 'Image';
  if (mimeType.startsWith('video/')) return 'Video';
  if (mimeType.startsWith('audio/')) return 'Audio';
  if (mimeType.includes('pdf') || mimeType.includes('document')) return 'Document';
  if (mimeType.includes('model') || mimeType.includes('3d')) return 'ThreeDModel';
  return 'Document'; // Default fallback
};

const getTypeSpecificFields = (asset, assetType) => {
  switch (assetType) {
    case 'Image':
      return {
        colors: asset.ai_tags?.filter(tag => tag.includes('color')) || [],
        qualityScore: asset.quality_score || 0,
        resolution: asset.metadata?.resolution || 'unknown',
        fileFormat: asset.mime_type.split('/')[1].toUpperCase()
      };
    case 'Video':
      return {
        duration: asset.metadata?.duration || 0,
        resolution: asset.metadata?.resolution || 'unknown',
        fileFormat: asset.mime_type.split('/')[1].toUpperCase()
      };
    case 'Audio':
      return {
        duration: asset.metadata?.duration || 0,
        fileFormat: asset.mime_type.split('/')[1].toUpperCase()
      };
    case 'Document':
      return {
        fullText: asset.metadata?.extracted_text || '',
        fileFormat: asset.mime_type.split('/')[1].toUpperCase()
      };
    case 'ThreeDModel':
      return {
        polygonCount: asset.metadata?.polygon_count || 0,
        textureCount: asset.metadata?.texture_count || 0,
        fileFormat: asset.file_name.split('.').pop().toUpperCase()
      };
    default:
      return {};
  }
};

            <h3 class="content-section__subtitle">Weaviate Performance Metrics</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Operation Type</th>
                        <th scope="col">Average Response Time</th>
                        <th scope="col">Throughput (ops/sec)</th>
                        <th scope="col">Memory Usage</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Operation Type">Vector Search (Single Class)</td><td data-label="Average Response Time">45-80ms</td><td data-label="Throughput (ops/sec)">1,200</td><td data-label="Memory Usage">2.1GB</td></tr>
                    <tr><td data-label="Operation Type">Cross-Class Search</td><td data-label="Average Response Time">120-200ms</td><td data-label="Throughput (ops/sec)">500</td><td data-label="Memory Usage">3.8GB</td></tr>
                    <tr><td data-label="Operation Type">Hybrid Search</td><td data-label="Average Response Time">60-110ms</td><td data-label="Throughput (ops/sec)">900</td><td data-label="Memory Usage">2.5GB</td></tr>
                    <tr><td data-label="Operation Type">Asset Ingestion</td><td data-label="Average Response Time">200-350ms</td><td data-label="Throughput (ops/sec)">300</td><td data-label="Memory Usage">1.9GB</td></tr>
                    <tr><td data-label="Operation Type">Batch Ingestion (100 items)</td><td data-label="Average Response Time">2.5-4.2s</td><td data-label="Throughput (ops/sec)">2,400</td><td data-label="Memory Usage">4.2GB</td></tr>
                </tbody>
            </table>

            <h3 class="content-section__subtitle">Weaviate Integration with n8n Workflows</h3>
            <pre class="code-block">
// n8n Workflow Integration for Weaviate
const n8nWeaviateIntegration = {
  // Trigger: New asset processed in PostgreSQL
  onAssetProcessed: async (assetId) => {
    try {
      // Fetch asset data from PostgreSQL
      const asset = await fetchAssetFromPostgreSQL(assetId);
      
      // Determine asset type and prepare Weaviate data
      const assetType = determineAssetType(asset.mime_type);
      const weaviateData = prepareWeaviateData(asset, assetType);
      
      // Ingest to Weaviate
      const result = await ingestAsset(weaviateData, assetType);
      
      // Update PostgreSQL with Weaviate object ID
      await updatePostgreSQLWithWeaviateId(assetId, result.id);
      
      // Send notification
      await sendNotification(`Asset ${asset.file_name} indexed in Weaviate`);
      
      return { success: true, weaviateId: result.id };
    } catch (error) {
      console.error('Weaviate integration error:', error);
      await logError(assetId, error.message);
      return { success: false, error: error.message };
    }
  },
  
  // Batch processing for existing assets
  batchProcessExistingAssets: async () => {
    const unprocessedAssets = await getUnprocessedAssets();
    const batchSize = 50;
    
    for (let i = 0; i < unprocessedAssets.length; i += batchSize) {
      const batch = unprocessedAssets.slice(i, i + batchSize);
      const weaviateBatch = batch.map(asset => ({
        type: determineAssetType(asset.mime_type),
        data: prepareWeaviateData(asset, determineAssetType(asset.mime_type)),
        embedding: null // Will be generated by Weaviate
      }));
      
      await batchIngestAssets(weaviateBatch);
      console.log(`Processed batch ${Math.floor(i/batchSize) + 1}`);
    }
  }
};

// Helper function for n8n webhook
const handleWeaviateWebhook = async (req, res) => {
  const { action, assetId, data } = req.body;
  
  switch (action) {
    case 'ingest':
      const result = await n8nWeaviateIntegration.onAssetProcessed(assetId);
      res.json(result);
      break;
    case 'search':
      const searchResults = await searchAllAssets(data.query, data.limit || 10);
      res.json(searchResults);
      break;
    case 'batch_process':
      await n8nWeaviateIntegration.batchProcessExistingAssets();
      res.json({ success: true, message: 'Batch processing initiated' });
      break;
    default:
      res.status(400).json({ error: 'Unknown action' });
  }
};
            </pre>
        </section>

        <section id="event-processing" class="content-section">
            <h2 class="content-section__title">Event Processing Pipeline</h2>

            <h3 class="content-section__subtitle">RabbitMQ Exchange and Queue Architecture</h3>
            <div class="mermaid-container1">
                <div class="mermaid">
flowchart LR
  %% ----------------------- RabbitMQ -----------------------
  subgraph RabbitMQ
    %% Exchanges
    E[(drive.events<br/>EXCHANGE)]
    FP[(file.preprocess<br/>EXCHANGE)]
    AE[(asset.events<br/>EXCHANGE)]
    PT[(plm.transitions<br/>EXCHANGE)]
    SN[(system.notifications<br/>EXCHANGE)]

    %% Queues
    subgraph Queues
      Qdoc[(document_ingest_queue)]
      Qimg[(image_ingest_queue)]
      Qvid[(video_ingest_queue)]
      Q3d[(3d_model_ingest_queue)]
      Qaud[(audio_ingest_queue)]
      Qapp[(approval_request_queue)]
      Qpre[(file.preprocess_queue)]
      Qdraft[(draft.processor)]
      QgateA[(gateA.reviewer)]
      Qproto[(prototype.manager)]
      Qppap[(ppap.quality)]
      Qprod[(production.publisher)]
      Qnotif[(notifications.googlechat)]
    end

    %% drive.events bindings
    E -->|file.document.*| Qdoc
    E -->|file.approved.image.*| Qimg
    E -->|file.approved.video.*| Qvid
    E -->|file.approved.3d_model.*| Q3d
    E -->|file.approved.audio.*| Qaud

    E -->|file.image.created| Qapp
    E -->|file.video.created| Qapp
    E -->|file.audio.created| Qapp

    %% New exchanges' bindings
    FP -->|file| Qpre

    %% asset.events bindings
    AE -->|asset.*.*.created| Qdraft
    AE -->|asset.*.*.approved| QgateA
    AE -->|asset.solidworks.*.created| Qproto
    AE -->|asset.*.*.stage.ppap| Qppap
    AE -->|asset.*.*.stage.production| Qprod

    %% plm.transitions bindings
    PT -->|plm.stage.gateA| QgateA
    PT -->|plm.stage.prototype| Qproto
    PT -->|plm.stage.ppap| Qppap
    PT -->|plm.stage.production| Qprod

    %% system.notifications bindings
    SN -->|#| Qnotif
  end

  %% ----------------------- n8n Workflows ------------------
  subgraph n8n Workflows
    Qdoc  --> Wdoc[Document&nbsp;Ingest]
    Qimg  --> Wimg[Image&nbsp;Ingest]
    Qvid  --> Wvid[Video&nbsp;Ingest]
    Q3d   --> W3d[3D&nbsp;Model&nbsp;Ingest]
    Qaud  --> Waud[Audio&nbsp;Ingest]
    Qapp  --> Wapp[Approval&nbsp;Request]
    Qpre  --> Wpre[File&nbsp;Preprocess]
    Qdraft --> Wdraft[PLM&nbsp;Draft&nbsp;Processor]
    QgateA --> WgateA[PLM&nbsp;Gate-A&nbsp;Reviewer]
    Qproto --> Wproto[PLM&nbsp;Prototype&nbsp;Manager]
    Qppap --> Wppap[PLM&nbsp;PPAP&nbsp;Quality]
    Qprod --> Wprod[PLM&nbsp;Production&nbsp;Publisher]
    Qnotif --> Wnotif[System&nbsp;Notifications]
  end
                </div>
            </div>
            </div>

            <h3 class="content-section__subtitle">Event Routing Strategy</h3>
            <p>The RabbitMQ exchange system implements intelligent routing based on file types and processing status. Documents are automatically processed without approval requirements, while media files require approval before proceeding to specialized ingest workflows. This design ensures appropriate quality control while maintaining processing efficiency for different asset types.</p>

            <h3 class="content-section__subtitle">Queue Management</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Queue Name</th>
                        <th scope="col">Routing Key Pattern</th>
                        <th scope="col">Processing Type</th>
                        <th scope="col">Approval Required</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Queue Name">document_ingest_queue</td><td data-label="Routing Key Pattern">file.document.*</td><td data-label="Processing Type">Automatic OCR</td><td data-label="Approval Required">No</td></tr>
                    <tr><td data-label="Queue Name">image_ingest_queue</td><td data-label="Routing Key Pattern">file.approved.image.*</td><td data-label="Processing Type">AI Analysis + CDN</td><td data-label="Approval Required">Yes</td></tr>
                    <tr><td data-label="Queue Name">video_ingest_queue</td><td data-label="Routing Key Pattern">file.approved.video.*</td><td data-label="Processing Type">Video Processing</td><td data-label="Approval Required">Yes</td></tr>
                    <tr><td data-label="Queue Name">3d_model_ingest_queue</td><td data-label="Routing Key Pattern">file.approved.3d_model.*</td><td data-label="Processing Type">3D Asset Processing</td><td data-label="Approval Required">Yes</td></tr>
                    <tr><td data-label="Queue Name">audio_ingest_queue</td><td data-label="Routing Key Pattern">file.approved.audio.*</td><td data-label="Processing Type">Audio Processing</td><td data-label="Approval Required">Yes</td></tr>
                    <tr><td data-label="Queue Name">approval_request_queue</td><td data-label="Routing Key Pattern">file.*.created</td><td data-label="Processing Type">Approval Workflow</td><td data-label="Approval Required">N/A</td></tr>
                    <tr><td data-label="Queue Name">publish_market_queue</td><td data-label="Routing Key Pattern">file.publish.*</td><td data-label="Processing Type">Multi-platform Sync</td><td data-label="Approval Required">No</td></tr>
                </tbody>
            </table>

            <h3 class="content-section__subtitle">Message Flow Architecture</h3>
            <pre class="code-block">
1. Google Drive File Event
   ‚Üì
2. Pub/Sub Notification
   ‚Üì
3. Cloud Function Processing
   ‚Üì
4. RabbitMQ Message Publication
   ‚Üì
5. Queue Routing (based on file type)
   ‚Üì
6. n8n Workflow Trigger
   ‚Üì
7. Processing Pipeline Execution
   ‚Üì
8. Database Status Update
   ‚Üì
9. Downstream System Notification
            </pre>
        </section>

        <section id="approval-workflows" class="content-section">
            <h2 class="content-section__title">Approval Workflows</h2>

            <h3 class="content-section__subtitle">Approval Request Creation Flow</h3>
            <div class="mermaid-container1">
                <div class="mermaid">
flowchart TB
  subgraph "Create Approval Request"
    B[üê∞ RabbitMQ Trigger]
    B --> C[üõ†Ô∏è Parse Drive Event]
    C --> D[üí¨ Post Google Chat Card]
    C --> E[üìÑ Append to Google Sheet]
    
    D --> F[üåê HTTP Webhook<br/>await button click]

  end

  H[üõ†Ô∏è Parse Chat-Callback/Sheet Update]

  H --> I{‚úÖ Approved?}
  I -->|Yes| J[üì§ Publish `approved.file.media.`]
  I -->|No| K[üì§ Publish `rejected.file.media.`]

  J --> L[üöÄ Media Ingest Workflow]
  K --> M[‚ùå Rejection Handler]
                </div>
            </div>

            <h3 class="content-section__subtitle">Multi-Channel Approval System</h3>
            <p>The approval system implements a sophisticated multi-channel approach that accommodates different team preferences and workflows. Team members can approve or reject media assets through Google Chat interactive cards, Google Sheets collaborative editing, or Google Forms structured submissions. This flexibility ensures that approval workflows integrate seamlessly with existing team communication patterns.</p>

            <h3 class="content-section__subtitle">Approval Workflow Implementation</h3>
            <p>Based on the n8n workflow analysis, the Create Approval Request workflow demonstrates sophisticated integration between RabbitMQ message consumption, PostgreSQL database operations, and multi-channel notification systems. The workflow parses incoming drive events, creates comprehensive database records, and simultaneously posts interactive approval cards to Google Chat while logging requests in Google Sheets for audit trail purposes.</p>

            <h3 class="content-section__subtitle">Decision Processing Logic</h3>
            <p>The Receive Approval workflow handles decision consolidation from multiple sources with intelligent conflict resolution. When approvals are received from multiple channels, the system prioritizes the most recent decision while maintaining complete audit trails. File status updates in PostgreSQL trigger automatic Google Drive file organization, moving rejected files to designated folders and approved files to processing queues.</p>

            <h3 class="content-section__subtitle">Approval Status Matrix</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Approval Channel</th>
                        <th scope="col">Response Time</th>
                        <th scope="col">Automation Level</th>
                        <th scope="col">Audit Trail</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Approval Channel">Google Chat Cards</td><td data-label="Response Time">Real-time</td><td data-label="Automation Level">Fully Automated</td><td data-label="Audit Trail">Complete</td></tr>
                    <tr><td data-label="Approval Channel">Google Sheets</td><td data-label="Response Time">Near Real-time</td><td data-label="Automation Level">Semi-Automated</td><td data-label="Audit Trail">Complete</td></tr>
                    <tr><td data-label="Approval Channel">Google Forms</td><td data-label="Response Time">Batch Processing</td><td data-label="Automation Level">Manual Entry</td><td data-label="Audit Trail">Complete</td></tr>
                    <tr><td data-label="Approval Channel">System Auto-Approval</td><td data-label="Response Time">Immediate</td><td data-label="Automation Level">Fully Automated</td><td data-label="Audit Trail">System Generated</td></tr>
                </tbody>
            </table>
        </section>

        <section id="media-processing" class="content-section">
            <h2 class="content-section__title">Media Processing</h2>

            <h3 class="content-section__subtitle">Document Processing Pipeline</h3>
            <p>The document ingest workflow implements a sophisticated dual-engine OCR approach using both Mistral AI and AWS Textract for maximum text extraction accuracy. Documents automatically bypass the approval process and proceed directly to processing, where extracted text is indexed in both PostgreSQL for structured queries and Weaviate for semantic search capabilities.</p>

            <h3 class="content-section__subtitle">AI-Powered Media Analysis</h3>
            <p>Media assets undergo comprehensive AI analysis using OpenAI Vision API for content understanding and automatic tagging. The system generates detailed metadata including product type identification, color palette extraction, material recognition, style classification, and quality assessment scoring. This metadata is stored as vectors in Weaviate to enable sophisticated semantic search across the entire media repository.</p>

            <h3 class="content-section__subtitle">CDN Optimization Strategy</h3>
            <p>Cloudinary integration provides automatic format optimization and global distribution through intelligent CDN routing. The system generates multiple format variants including thumbnails for quick previews, web-optimized versions for online display, social media formats for marketing distribution, and print-quality versions for production materials. All transformations are performed automatically based on predefined optimization rules.</p>

            <h3 class="content-section__subtitle">Processing Status Tracking</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Processing Stage</th>
                        <th scope="col">Status Code</th>
                        <th scope="col">Duration (Avg)</th>
                        <th scope="col">Success Rate</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Processing Stage">Initial Upload</td><td data-label="Status Code">PENDING</td><td data-label="Duration (Avg)">< 1 second</td><td data-label="Success Rate">99.9%</td></tr>
                    <tr><td data-label="Processing Stage">AI Analysis</td><td data-label="Status Code">ANALYZING</td><td data-label="Duration (Avg)">15-30 seconds</td><td data-label="Success Rate">98.5%</td></tr>
                    <tr><td data-label="Processing Stage">Vector Storage</td><td data-label="Status Code">INDEXING</td><td data-label="Duration (Avg)">5-10 seconds</td><td data-label="Success Rate">99.2%</td></tr>
                    <tr><td data-label="Processing Stage">CDN Upload</td><td data-label="Status Code">OPTIMIZING</td><td data-label="Duration (Avg)">10-20 seconds</td><td data-label="Success Rate">99.7%</td></tr>
                    <tr><td data-label="Processing Stage">Format Generation</td><td data-label="Status Code">TRANSFORMING</td><td data-label="Duration (Avg)">20-45 seconds</td><td data-label="Success Rate">99.1%</td></tr>
                    <tr><td data-label="Processing Stage">Database Update</td><td data-label="Status Code">FINALIZING</td><td data-label="Duration (Avg)">2-5 seconds</td><td data-label="Success Rate">99.8%</td></tr>
                    <tr><td data-label="Processing Stage">Processing Complete</td><td data-label="Status Code">OPTIMIZED</td><td data-label="Duration (Avg)">N/A</td><td data-label="Success Rate">98.9%</td></tr>
                </tbody>
            </table>
        </section>

        <section id="plm-stages" class="content-section">
            <h2 class="content-section__title">PLM Stages Documentation</h2>

            <h3 class="content-section__subtitle">Overview of PLM Stages</h3>
            <p>The Product Lifecycle Management (PLM) system is designed to manage the entire lifecycle of a product from its conception, through design and manufacture, to service and disposal. Each stage is characterized by specific activities, deliverables, and approval processes, ensuring a structured and controlled progression of product development.</p>

            <h3 class="content-section__subtitle">Draft Stage (Concept)</h3>
            <p>The Draft stage is the initial phase where new product ideas and concepts are captured. Any new file uploaded to the product folder is automatically considered a draft. This stage focuses on early design intelligence, including research, sketches, and concept CAD. The system automatically registers each new concept as a PLM item in the database and creates a corresponding Engineering Change Order (ECO) in ERPNext.</p>
            <pre class="code-block">
<b>Event:</b> asset.*.*.created
<b>Producer:</b> Google Drive Webhook ‚Üí RabbitMQ
<b>Consumer:</b> draft.processor queue
<b>Actions:</b>
1. Insert Product Draft record in PostgreSQL (media_assets table), setting plm_stage=\'Draft\'.
2. Call ERPNext API to create a Concept ECO.
3. Send Google Chat notification: \"New draft asset uploaded: &lt;filename&gt; (ECO #‚Ä¶)\".
            </pre>

            <h3 class="content-section__subtitle">Gate-A Approval (Feasibility Review)</h3>
            <p>The Gate-A stage automates the submission of concept deliverables for review and captures approval/rejection status. Once an ECO is approved in ERPNext, the asset transitions to this stage. The system can optionally perform OCR and compliance checks on attached documents. Upon approval, an Approval Certificate is generated, detailing the approver, date, and specific approval data from the workflow.</p>
            <pre class="code-block">
<b>Event:</b> asset.*.*.approved
<b>Trigger:</b> ERPNext ECO transitions to \"Approved\" ‚Üí MCP server publishes asset.*.*.approved.
<b>Consumer:</b> gateA.reviewer queue
<b>Actions:</b>
1. Update media_assets.plm_stage=\'Gate-A\'.
2. Generate an Approval Document (PDF) with approver name, timestamp, and approval data.
3. Store document link in ERPNext ECO as an attachment.
4. Notify stakeholders via Google Chat.
            </pre>

            <h3 class="content-section__subtitle">Prototype Stage</h3>
            <p>Following Gate-A approval, the Prototype stage initiates the physical prototyping process. This includes Bill of Material (BOM) generation in ERPNext and tracking of prototype assets. For SOLIDWORKS files, the system automatically extracts metadata using the SOLIDWORKS API and creates work orders in ERPNext. An Approval Certificate is generated upon successful completion of this stage, confirming the prototype's readiness.</p>
            <pre class="code-block">
<b>Event:</b> asset.solidworks.*.created
<b>Producer:</b> Designer uploads .SLDPRT or .SLDASM ‚Üí RabbitMQ
<b>Consumer:</b> prototype.manager queue
<b>Actions:</b>
1. Extract custom properties (part number, material) using SOLIDWORKS API and push to PostgreSQL.
2. Call ERPNext to auto-generate a Prototype Work Order with BOM.
3. Update media_assets with plm_stage=\'Prototype\'.
4. Generate an Approval Certificate for prototype readiness.
            </pre>

            <h3 class="content-section__subtitle">PPAP (Production Part Approval Process) Stage</h3>
            <p>The PPAP stage validates production readiness through quality checks, supplier sign-offs, and sample approvals. Once prototype samples are tested and ERPNext QC Inspection is \"Accepted\", the system publishes an event to trigger this stage. Upon successful completion, a signed PPAP Certificate PDF is generated, confirming material readiness and quality compliance. This certificate includes details of the quality checks, approvers, and dates.</p>
            <pre class="code-block">
<b>Event:</b> asset.*.*.stage.ppap
<b>Trigger:</b> ERPNext QC Inspection \"Accepted\" ‚Üí MCP publishes asset.*.*.stage.ppap.
<b>Consumer:</b> ppap.quality queue
<b>Actions:</b>
1. Create/Update ERPNext Quality Inspection records.
2. Upon pass, mark plm_stage=\'PPAP\' in PostgreSQL.
3. Generate a signed PPAP Certificate PDF with quality data and approver details.
4. Notify procurement and operations.
            </pre>

            <h3 class="content-section__subtitle">Production & Publishing Stage</h3>
            <p>The final stage, Production & Publishing, is triggered when ERPNext signals \"Material Ready\". In this phase, the final SKU is generated using n8n automation pipelines, leveraging Google Drive folder names and file names. The product is then published live on WooCommerce and Webflow. All finalized media is archived, and a final Approval Certificate is generated, confirming the product is ready for market and detailing all publishing information.</p>
            <pre class="code-block">
<b>Event:</b> asset.*.*.stage.production
<b>Trigger:</b> ERPNext Material Ready webhook ‚Üí MCP publishes asset.*.*.stage.production.
<b>Consumer:</b> production.publisher queue
<b>Actions:</b>
1. Generate final SKU in ERPNext via n8n automation (parsing Drive metadata and file name).
2. Push SKU, pricing, stock levels to WooCommerce via its REST API.
3. Create or update the Webflow CMS item and trigger a site publish via Webflow API.
4. Archive media under /Processed-Media/ERP-Integrated/ and confirm backup.
5. Generate a final Approval Certificate for production readiness and publishing details.
6. Broadcast \"Product &lt;SKU&gt; is live\" to Google Chat.
            </pre>
        </section>

        <section id="external-integrations" class="content-section">
            <h2 class="content-section__title">External Integrations</h2>

            <h3 class="content-section__subtitle">ERPNext Synchronization</h3>
            <p>The ERPNext integration maintains bidirectional synchronization between the media repository and product management system. Scheduled workflows update product records with optimized media URLs, ensuring that inventory management, pricing, and bill-of-materials (BOM) tracking remain synchronized with the latest approved media assets. The integration supports both individual product updates and bulk synchronization operations.</p>

            <h3 class="content-section__subtitle">Cloudinary Configuration</h3>
            <pre class="code-block">
// DSI Upload Preset Configuration
{
  "name": "dsi-product-media",
  "unsigned": false,
  "folder": "dsi/products",
  "allowed_formats": ["jpg", "png", "webp", "mp4", "mov"],
  "transformation": [
    {
      "quality": "auto:best",
      "fetch_format": "auto"
    }
  ],
  "eager": [
    {
      "width": 150,
      "height": 150,
      "crop": "thumb",
      "gravity": "auto"
    },
    {
      "width": 800,
      "quality": "auto",
      "fetch_format": "auto"
    },
    {
      "width": 1200,
      "height": 630,
      "crop": "fill",
      "gravity": "auto"
    }
  ],
  "eager_async": true,
  "auto_tagging": 80,
  "categorization": "google_tagging",
  "detection": "adv_face"
}
            </pre>

            <h3 class="content-section__subtitle">WooCommerce & Webflow Integration</h3>
            <p>The publication workflow orchestrates final asset distribution across WooCommerce for eCommerce operations and Webflow for marketing website management. The system ensures synchronized product information and media assets across all business platforms, with automatic URL updates and format optimization for each platform's specific requirements.</p>

            <h3 class="content-section__subtitle">Integration Status Monitoring</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Integration Target</th>
                        <th scope="col">Sync Frequency</th>
                        <th scope="col">Data Direction</th>
                        <th scope="col">Error Handling</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Integration Target">ERPNext</td><td data-label="Sync Frequency">Daily (2 AM)</td><td data-label="Data Direction">Bidirectional</td><td data-label="Error Handling">Retry + Alert</td></tr>
                    <tr><td data-label="Integration Target">Cloudinary</td><td data-label="Sync Frequency">Real-time</td><td data-label="Data Direction">Upload Only</td><td data-label="Error Handling">Queue + Retry</td></tr>
                    <tr><td data-label="Integration Target">WooCommerce</td><td data-label="Sync Frequency">Hourly</td><td data-label="Data Direction">Push Only</td><td data-label="Error Handling">Batch Retry</td></tr>
                    <tr><td data-label="Integration Target">Webflow</td><td data-label="Sync Frequency">On Approval</td><td data-label="Data Direction">Push Only</td><td data-label="Error Handling">Manual Review</td></tr>
                    <tr><td data-label="Integration Target">Google Drive</td><td data-label="Sync Frequency">Real-time</td><td data-label="Data Direction">Bidirectional</td><td data-label="Error Handling">Webhook Retry</td></tr>
                </tbody>
            </table>
        </section>

        <section id="ai-intelligence" class="content-section">
            <h2 class="content-section__title">AI & Intelligence</h2>

            <h3 class="content-section__subtitle">OpenAI Vision Integration</h3>
            <pre class="code-block">
// Image Analysis Configuration
const analyzeImage = async (imageBuffer) => {
  const prompt = `
    Analyze this luxury product image and extract:
    1. Product type and category
    2. Primary colors (hex codes if possible)
    3. Materials and textures visible
    4. Style descriptors (modern, classic, etc.)
    5. Target audience demographics
    6. Suggested marketing keywords
    7. Quality assessment (1-10)
    
    Return as structured JSON.
  `;
  
  const response = await openai.chat.completions.create({
    model: "gpt-4-vision-preview",
    messages: [{
      role: "user",
      content: [
        { type: "text", text: prompt },
        { 
          type: "image_url", 
          image_url: {
            url: `data:image/jpeg;base64,${imageBuffer.toString('base64')}`
          }
        }
      ]
    }],
    max_tokens: 500
  });
  
  return JSON.parse(response.choices[0].message.content);
};
            </pre>

            <h3 class="content-section__subtitle">Weaviate Vector Storage</h3>
            <p>Weaviate serves as the semantic search engine for the entire media repository, storing AI-generated embeddings that enable sophisticated content discovery. The system automatically generates vector representations of all media assets, allowing for queries like "find all gold jewelry with modern styling" or "locate images similar to this luxury fragrance bottle design."</p>

            <h3 class="content-section__subtitle">OCR Services Architecture</h3>
            <div class="mermaid-container5">
                <div class="mermaid">
graph TB
    subgraph DI["Document Input"]
        PDF["üìÑ PDF Documents<br>Invoices, Contracts"]
        IMG["üñºÔ∏è Scanned Images<br>Photos, Scans"]
    end
    
    subgraph OES["OCR Edge Service"]
        PRE["‚öôÔ∏è Pre-processor<br>Tesseract 5.4"]
        GCV["üëÅÔ∏è Google Cloud Vision<br>Multi-language OCR"]
        TXT["üìä AWS Textract<br>Forms & Tables"]
        MRG["üîÑ Result Merger<br>Textractor"]
    end
    
    subgraph OP["Output"]
        JSON["üìã Structured JSON<br>Extracted Data"]
        TXT2["üìù Plain Text<br>Full Content"]
        META["üè∑Ô∏è Metadata<br>Key Terms"]
    end
    
    subgraph INT["Integration"]
        WV["üîç Weaviate<br>Vector Search"]
        ERP["üìä ERPNext<br>Document Index"]
        CDN["üåê Cloudinary<br>Storage"]
        CHAT["üí¨ Google Chat<br>Notifications"]
    end
    
    PDF --> PRE
    IMG --> PRE
    PRE --> GCV
    PRE --> TXT
    GCV --> MRG
    TXT --> MRG
    MRG --> JSON
    MRG --> TXT2
    MRG --> META
    JSON --> WV
    JSON --> ERP
    JSON --> CDN
    META --> CHAT
                </div>
            </div>

            <h3 class="content-section__subtitle">Model Context Protocol (MCP)</h3>
            <p>The Model Context Protocol provides standardized metadata management across all systems in the DSI ecosystem. MCP ensures consistency and enables AI agents to understand and manage content intelligently through structured context tracking and version control. The protocol integrates deeply with RabbitMQ to provide event-driven context propagation, where each message carries MCP metadata that allows services to understand the full context of an asset's lifecycle. When deployed on Kubernetes, MCP-enabled Service Agents orchestrate complex workflows by subscribing to specific RabbitMQ exchanges and queues, processing messages based on their MCP context, and publishing enriched events back to the system. These Service Agents run as lightweight Kubernetes deployments, each responsible for specific aspects of the media processing pipeline, from AI analysis to format conversion. The agents automatically scale based on queue depth and processing load, ensuring efficient resource utilization while maintaining context consistency across all distributed services.</p>

            <h3 class="content-section__subtitle">AI Processing Metrics</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">AI Service</th>
                        <th scope="col">Processing Time</th>
                        <th scope="col">Accuracy Rate</th>
                        <th scope="col">Cost per Asset</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="AI Service">OpenAI Vision Analysis</td><td data-label="Processing Time">15-30 seconds</td><td data-label="Accuracy Rate">94.2%</td><td data-label="Cost per Asset">$0.08</td></tr>
                    <tr><td data-label="AI Service">Weaviate Vector Generation</td><td data-label="Processing Time">5-10 seconds</td><td data-label="Accuracy Rate">98.7%</td><td data-label="Cost per Asset">$0.02</td></tr>
                    <tr><td data-label="AI Service">OCR Text Extraction</td><td data-label="Processing Time">10-25 seconds</td><td data-label="Accuracy Rate">96.8%</td><td data-label="Cost per Asset">$0.05</td></tr>
                    <tr><td data-label="AI Service">Quality Assessment</td><td data-label="Processing Time">8-15 seconds</td><td data-label="Accuracy Rate">91.5%</td><td data-label="Cost per Asset">$0.03</td></tr>
                </tbody>
            </table>
        </section>

        <section id="deployment-guide" class="content-section">
            <h2 class="content-section__title">Deployment Guide</h2>

            <h3 class="content-section__subtitle">Docker Compose Configuration</h3>
            <pre class="code-block">
version: '3.8'

services:
  n8n:
    image: n8nio/n8n:latest
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD}
      - N8N_HOST=n8n.designershaik.com
      - N8N_PROTOCOL=https
      - N8N_WEBHOOK_BASE_URL=https://n8n.designershaik.com
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_USER=n8n_user
      - DB_POSTGRESDB_PASSWORD=${DB_PASSWORD}
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      - postgres
      - redis
      - rabbitmq

  postgres:
    image: postgres:15
    restart: unless-stopped
    environment:
      - POSTGRES_USER=dsi_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=dsi_media
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  weaviate:
    image: semitechnologies/weaviate:latest
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=false
      - AUTHENTICATION_APIKEY_ENABLED=true
      - AUTHENTICATION_APIKEY_ALLOWED_KEYS=${WEAVIATE_API_KEY}
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - DEFAULT_VECTORIZER_MODULE=text2vec-openai
      - ENABLE_MODULES=text2vec-openai
      - OPENAI_APIKEY=${OPENAI_API_KEY}
    volumes:
      - weaviate_data:/var/lib/weaviate

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data

  rabbitmq:
    image: rabbitmq:3-management
    restart: unless-stopped
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq

volumes:
  n8n_data:
  postgres_data:
  weaviate_data:
  redis_data:
  rabbitmq_data:
            </pre>

            <h3 class="content-section__subtitle">Environment Configuration</h3>
            <pre class="code-block">
# .env file
N8N_USER=admin
N8N_PASSWORD=secure_password_here
DB_PASSWORD=postgres_password_here
POSTGRES_PASSWORD=postgres_password_here
WEAVIATE_API_KEY=weaviate_api_key_here
OPENAI_API_KEY=sk-proj-...
REDIS_PASSWORD=redis_password_here
RABBITMQ_USER=dsi_admin
RABBITMQ_PASSWORD=rabbitmq_password_here

# Google Cloud
GCP_PROJECT_ID=n8n-drive-integration-465208
GCP_SERVICE_ACCOUNT_EMAIL=dsi-drive-watcher@n8n-drive-integration-465208.iam.gserviceaccount.com

# Cloudinary
CLOUDINARY_CLOUD_NAME=designershaik
CLOUDINARY_API_KEY=your_api_key
CLOUDINARY_API_SECRET=your_api_secret

# ERPNext
ERPNEXT_URL=https://erp.designershaik.com
ERPNEXT_API_KEY=your_api_key
ERPNEXT_API_SECRET=your_api_secret
            </pre>

            <h3 class="content-section__subtitle">Deployment Options</h3>
            <div class="cost-comparison">
                <div class="cost-card">
                    <h4 class="cost-card__title">Development Environment</h4>
                    <p class="cost-card__price">~$12/month</p>
                    <ul>
                        <li><strong>Components:</strong> All services on Docker containers, single VPS</li>
                        <li><strong>Ideal For:</strong> Development, testing, small teams</li>
                        <li><strong>Complexity:</strong> Low</li>
                    </ul>
                </div>
                <div class="cost-card">
                    <h4 class="cost-card__title">Production Environment</h4>
                    <p class="cost-card__price">~$60/month</p>
                    <ul>
                        <li><strong>Components:</strong> Kubernetes cluster, auto-scaling, load balancing</li>
                        <li><strong>Ideal For:</strong> Production scale, high availability</li>
                        <li><strong>Complexity:</strong> Medium</li>
                    </ul>
                </div>
                <div class="cost-card">
                    <h4 class="cost-card__title">Enterprise Environment</h4>
                    <p class="cost-card__price">~$200/month</p>
                    <ul>
                        <li><strong>Components:</strong> Multi-region deployment, enterprise SLA</li>
                        <li><strong>Ideal For:</strong> Enterprise requirements, compliance</li>
                        <li><strong>Complexity:</strong> High</li>
                    </ul>
                </div>
            </div>
        </section>

        <section id="monitoring-testing" class="content-section">
            <h2 class="content-section__title">Monitoring & Testing</h2>

            <h3 class="content-section__subtitle">Testing Protocol</h3>
            <div class="mermaid-container">
                <div class="mermaid">
flowchart TB
    Upload["1Ô∏è‚É£ Test Upload<br>Initiate"]
    Queue["2Ô∏è‚É£ RabbitMQ Queue<br>Processing"]
    Logs["3Ô∏è‚É£ Verify Logs<br>Execution"]
    Process["4Ô∏è‚É£ Process Validation<br>Workflow"]
    Output["5Ô∏è‚É£ Output Check<br>Results"]
    Monitor["6Ô∏è‚É£ Monitoring Validation<br>Metrics"]
    
    Upload --> Queue
    Queue --> Logs
    Logs --> Process
    Process --> Output
    Output --> Monitor
    
    Upload -.-> UploadDetail["üì§ Upload test image to<br>Product-Images folder"]
    Queue -.-> QueueDetail["üê∞ Verify RabbitMQ queue<br>receives message"]
    Logs -.-> LogDetail["üìä Check n8n execution logs<br>for trigger activation"]
    Process -.-> ProcessDetail["‚úÖ Confirm workflow<br>processes file correctly"]
    Output -.-> OutputDetail["üåê Verify Cloudinary URLs in<br>Processed-Media folder"]
    Monitor -.-> MonitorDetail["üìà Check monitoring dashboard<br>for metrics updates"]
                </div>
            </div>

            <h3 class="content-section__subtitle">Performance Monitoring</h3>
            <p>The system implements comprehensive monitoring across all components including RabbitMQ queue depths, PostgreSQL query performance, Weaviate vector search response times, and Cloudinary transformation success rates. Automated alerts notify the team of any performance degradation or system failures through Google Chat integration.</p>

            <h3 class="content-section__subtitle">Key Performance Indicators</h3>
            <table class="data-table">
                <thead>
                    <tr>
                        <th scope="col">Metric Category</th>
                        <th scope="col">Key Indicator</th>
                        <th scope="col">Target Value</th>
                        <th scope="col">Alert Threshold</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td data-label="Metric Category">Queue Performance</td><td data-label="Key Indicator">Average Processing Time</td><td data-label="Target Value">< 2 minutes</td><td data-label="Alert Threshold">> 5 minutes</td></tr>
                    <tr><td data-label="Metric Category">Database Performance</td><td data-label="Key Indicator">Query Response Time</td><td data-label="Target Value">< 100ms</td><td data-label="Alert Threshold">> 500ms</td></tr>
                    <tr><td data-label="Metric Category">AI Processing</td><td data-label="Key Indicator">Success Rate</td><td data-label="Target Value">> 95%</td><td data-label="Alert Threshold">< 90%</td></tr>
                    <tr><td data-label="Metric Category">CDN Performance</td><td data-label="Key Indicator">Upload Success Rate</td><td data-label="Target Value">> 99%</td><td data-label="Alert Threshold">< 95%</td></tr>
                    <tr><td data-label="Metric Category">System Availability</td><td data-label="Key Indicator">Uptime Percentage</td><td data-label="Target Value">> 99.5%</td><td data-label="Alert Threshold">< 99%</td></tr>
                </tbody>
            </table>

            <h3 class="content-section__subtitle">Testing Procedures</h3>
            <pre class="code-block">
# End-to-End Testing Script
1. Upload test image to: DSI-Product-Repository/Palace-of-Fragrances/Classic-Range/Opulent-Shaik-Amethyst/Media/Images/

2. Monitor RabbitMQ queue:
   docker exec rabbitmq rabbitmqctl list_queues

3. Check n8n execution logs:
   docker logs dsi_n8n_1 --tail 100 -f

4. Verify PostgreSQL record:
   SELECT * FROM media_assets 
   WHERE drive_file_id = 'YOUR_FILE_ID';

5. Check Weaviate storage:
   curl http://localhost:8080/v1/objects?class=MediaAsset

6. Verify Cloudinary upload:
   https://res.cloudinary.com/designershaik/image/upload/dsi/...

7. Confirm ERP synchronization:
   Check ERPNext product record for updated media URLs

8. Validate monitoring metrics:
   Review dashboard for processing completion
            </pre>
        </section>

    </main>

    <footer class="footer">
        <p>¬© 2025 DSI Media Repository Architecture Guide</p>
        <p>This documentation provides complete technical specifications for deploying the DSI Media Repository Integration Architecture with automated workflows, intelligent approval systems, and comprehensive monitoring.</p>
        <p>Architecture: Event-driven RabbitMQ processing with multi-channel approval workflows, AI-powered content analysis, and automated cross-platform synchronization</p>
        <p>Maintained by: DSI Technical Team</p>
    </footer>

    <button class="back-to-top" id="backToTop" aria-label="Back to top">‚Üë</button>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js"></script>
<script src="script.js"></script>
</body>
</html>
